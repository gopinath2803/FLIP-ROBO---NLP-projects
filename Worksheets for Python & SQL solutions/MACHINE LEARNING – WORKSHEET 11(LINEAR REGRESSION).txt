		MACHINE LEARNING – WORKSHEET 11
		    (LINEAR REGRESSION)
In Q1 to Q8, only one option is correct, Choose the correct option:

1. What happens to R2 measure if we add a new feature?
A) remains same B) always increases
C) may or may not increase D) always decreases

ANS  B) always increases

2. The correct relationship between SST, SSR and SSE is given by:
 A) SSR = SST + SSE B) SST = SSR + SSE
C) SSE = SSR – SST D) None of the above

ANS  B) SST = SSR + SSE

3. Residuals in regression analysis can be defined as:
 A) difference between the actual value and the predicted value.
B) difference between the actual value and the mean of predicted value.
C) difference between the actual value and mean of dependent variable.
D) None of the above.

ANS  A) difference between the actual value and the predicted value

4. In a simple linear regression model, if we change the input variable by 1 unit, then how much output variable
will change?
A) By 1 B) No Change
C) By its slope D) None of the above

ANS  C) By its slope

5. If the coefficient of determination is equal to 1, then correlation coefficient:
A) must also be equal to 1 B) can be either -1 or 1
C) can be any value between -1 to 1 D) must be -1

ANS  B) can be either -1 or 1

6. Which of the following plot is best suited for the linear relationship of continuous variables?
A) Scatter plot B) Histograms
 C) Pie charts D) All of the above

ANS  A) Scatter plot

7. The ratio of MSR/MSE produces:
 A) t-statistics B) f-statistics
C) z-statistics D) None of the above.

ANS  B) f-statistics

8. Which of the following regularizations uses only L2 normalization for its penalty parameter?
A) Lasso B) Elastic Nets
C) Ridge D) All of the above

ANS  C) Ridge

In Q9 to Q11, more than one options are correct, Choose all the correct options:

9. Which of the following statement/s are true for best fitted line?
A) It shows the causal relationship between dependent and independent variables
B) It shows the positive or negative relation between dependent and independent variables
C) It always goes through origin
D) It is a straight line that is the best approximation of the given data sets

ANS  A) It shows the causal relationship between dependent and independent variables 
     B) It shows the positive or negative relation between dependent and independent variables 
     D) It is a straight line that is the best approximation of the given data sets


10. Regularizations helps in:
A) Reducing the training time B) Generalizing the test set
C) Automatic feature selection D) Grouping the data

ANS  C) Automatic feature selection

11. Linear regression can be implemented through:
A) Normal Equation B) Singular Value Decomposition
C) Parity checks D) nodes

ANS  A) Normal Equation

Q12 to Q15 are subjective answer type questions, Answer them briefly.

12. Explain R2 and adjusted R2 metrics?

R-squared, also known as the coefficient determination, defines the degree to which the variance in the 
dependent variable (or target) can be explained by the independent variable (features).
Similar to R-squared, the Adjusted R-squared measures the variation in the dependent variable (or target), 
explained by only the features which are helpful in making predictions. Unlike R-squared, the Adjusted 
R-squared would penalize you for adding features which are not useful for predicting the target.


13. Explain the cost function of linear regression?

In ML, cost functions are used to estimate how badly models are performing. Put simply, a cost function 
is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. 
This is typically expressed as a difference or distance between the predicted value and the actual value. 
The cost function (you may also see this referred to as loss or error.) can be estimated by iteratively running
the model to compare estimated predictions against “ground truth” — the known values of y.
The MSE in the case of Linear Regression is the cost function. It is simply the mean of the squared differences 
between predicted y and actual y (i.e. the residuals).


14. Differentiate SSE, SSR and SST.

SST: The sum of squares total, denoted SST, is the squared differences between the observed dependent variable and its mean.
SSR: The second term is the sum of squares due to regression, or SSR. It is the sum of the differences between the predicted 
     value and the mean of the dependent variable. 
SSE: The last term is the sum of squares error, or SSE. The error is the difference between the observed value and the predicted value.

15. What are the various evaluation metrics for linear regression?

•	Mean Squared Error (MSE)
•	Mean Absolute Error (MAE)
•	R-squared or Coefficient of Determination
•	Root Mean Squared Error (RMSE)